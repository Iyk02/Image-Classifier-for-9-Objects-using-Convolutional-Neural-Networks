# -*- coding: utf-8 -*-
"""Copy of train_eff_net_b3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rhPT8MRuDQ_ta4v9pwUYmwM_F0h1EfKg
"""

# For reproducible results

import os
os.environ['PYTHONHASHSEED']='100'

import random
random.seed(100)

import numpy as np
np.random.seed(100)

import tensorflow as tf
tf.random.set_seed(100)

## Importing the Datasets

!wget -nc http://ml_course.gordinmitya.ru/modified_public.zip
!unzip -o modified_public.zip

# xs and ys represent labeled training dataset
xs = './modified_public/xs.npy'
xs = np.load(xs)

ys = './modified_public/ys.npy'
ys = np.load(ys)

# xt represents unlabeled training dataset
xt = './modified_public/xt_modified.npy'
xt = np.load(xt)

## Splitting the training data
## 90% for training
## 10% for validation
from sklearn.model_selection import train_test_split

xs_train, xs_val, ys_train, ys_val = train_test_split(xs, ys, test_size=0.10, random_state=0)

print('Total number of training data:', len(xs))
num_train = len(xs_train)
print('Number of training examples:', num_train)
num_val = len(xs_val)
print('Number of validation examples:', num_val)
num_test = len(xt)
print('Total number of test data:', num_test)

class_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'horse', 'ship', 'truck']
class_names

import matplotlib.pyplot as plt
import math
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.layers import Conv2DTranspose, Reshape, Lambda, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Nadam, Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Function to visualize the data
def plot_sample(x, y, index):
    plt.figure(figsize=(15,2))
    plt.imshow(x[index])
    plt.xlabel(class_names[y[index]])

plot_sample(xs_train, ys_train, 100)

feature_extractor = tf.keras.applications.EfficientNetB3(include_top=False, input_shape=(224, 224, 3))

feature_extractor.summary()

len(feature_extractor.layers)

model = tf.keras.Sequential([
                             tf.keras.layers.experimental.preprocessing.Resizing(224, 224),
                             
                             tf.keras.layers.experimental.preprocessing.RandomFlip(),
                             tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),
                             
                             feature_extractor,
                             
                             tf.keras.layers.GlobalAveragePooling2D(),
                             tf.keras.layers.Dropout(0.2),
                             tf.keras.layers.Dense(9, activation=tf.nn.softmax)])

model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

history = model.fit(xs, ys,
                    epochs=8)

model.save('model.h5')